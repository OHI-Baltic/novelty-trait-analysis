---
title: "Novelty traits analysis - Preliminary"
format: html
editor: visual
---

## Introduction

This is a script to calculate novelty based on Ammar et al (2021), where novelty is the degree of dissimilarity of a system (or part of the system like functional groups of aa particular trophic grouyp) relative to a reference baseline (e.g. present time window). The example datasety originates from Pecuchet et al (2019).This script will be mainly used within the ICES Working Group of Integrated assessment of the Baltic Sea and is part of ToRa, lead by Marie Nordstrom and Thorsten Blenckner.   

This script is finally improved and provided by Elanore Campbell with support from Thorsten Blenckner




```{r libraries and directories}
#| code-fold: true
#| code-summary: Libraries and Directories
#| label: libraries-and-directories
#| message: false

## libraries
pkg <- c("here","vegan","analogue","EnvCpt","FD","dplyr","tidyr","ggplot2","zoo")
for(i in 1:length(pkg)){
  if(!(pkg[i] %in% installed.packages())){
    install.packages(pkg[i])
  }
}
for(i in 1:length(pkg)){
  library(pkg[i], character.only = TRUE)
}
  
## file.paths
data_dir <- here("data")
save_dir <- here("output")
```

```{r function for novelty and change}
#| code-fold: true
#| code-summary: Function for Novelty and Change
#| label: function-for-novelty-and-change

## Yosr's function from COMFORT.R script ----

## M is a dataframe or matrix with colname, in wide format
## with numeric columns for Year, and each trait or species, 
## or other 'feature' to evaluate novelty across; 
## and categorical column for Area subdivisions or gridcells

## baseline start and end should be years used to compare
## end_year is the last year to be compared to the baseline...

## year params could instead refer to binned time periods 
## (but needs to be an unbroken seq of integers, and be same column for baseline and future)

## method (Hellinger or EDLog) is the metric to use to evaluate dissimilarity distance

Novelty_function <- function(M, start_baseline, end_baseline, end_year, method){
  
  ## make sure M is a dataframe
  ## if it's a tibble, some [] subsetting stuff breaks
  M <- as.data.frame(M)
  
  ## columns with features of the space we're 
  ## calculating novelty over
  novelty_feature_cols <- which(!stringr::str_detect(names(M), "Year|Area|Bin"))
  novelty_feature_names <- names(M)[novelty_feature_cols]
  
  ## initialize a dataframe to hold results
  ## cma_dist will be the distance, found using the 'close modern analogues' function
  ## TS.ind are rownames of TS, to keep track how results align with M entries, w/ Years etc
  ## FS.ind are rownames of FS (baseline in comparisons)
  ## TS.area is the area for which novelty is evaluated 
  ## TS.year is the year for which novelty is evaluated
  ## FS.area is the area which has minimum dissimilarity, 
  ## i.e. area or gridcell which is the closest analog
  ## FS.year is the year in the baseline corresponding to the closest analog
  Ana_result <- data.frame(matrix(
    NA, nrow = 0, ncol = 7,
    dimnames = list(NULL, c(
      "cma_dist", "TS.ind", "FS.ind",
      "TS.area", "TS.year", "FS.area", "FS.year"
    ))
  ))
  Ana_result <- Ana_result |> 
    mutate(across(everything(), as.numeric)) |> 
    mutate(across(matches("area"), as.character))
  
  ## data which correspond to the years in the baseline
  ## using FS rownames to keep track of ...
  FS <- M[which(M[,"Year"] %in% c(start_baseline:end_baseline)),]
  FS.a <- FS[,novelty_feature_cols]
  names.fs <- rownames(FS.a)
  n.fs <- length(names.fs)
  
  if(nrow(FS) > 0){
    for(t in (end_baseline+1):end_year){
      ## select data for the year to be compared
      ## (year to calculate novelty for) (???)
      TS <- M[which(M[,"Year"] == t),]
      TS.a <- TS[,novelty_feature_cols]
      names.ts <- rownames(TS.a)
      n.ts <- length(names.ts)
      
      S.a <- rbind(TS.a, FS.a)
      
      if(nrow(TS) > 0){
        if(method == "Hellinger"){
          
          ## CALCULATE HELINGER DISTANCE ----
          ## helinger distance dissimilarity 
          ## based on the composition of the area
          
          ## make a matrix S.a2 where entries
          ## are normalized by the total value in the row
          ## (sum of all features we're calculating novelty across)
          S.a2 <- S.a
          for(i in 1:ncol(S.a)){
            for(j in 1:nrow(S.a)){
              S.a2[j,i] <- S.a[j,i]/rowSums(S.a[j,])
              ## why do we normalize by row sum instead of global max sum??
            }
          }
          S.a2[which(rowSums(TS.a) == FALSE),] <- 0 ## when would rowSums be false??
          ## using distance function from 'analogue' package
          ## 'flexibly calculates distance or dissimilarity measures...'
          S.d2 <- as.matrix(distance(S.a2, method = "chord", dist = T))
          
          S.dd <- S.d2
          S.dd <- matrix(
            ## keep only rows corresponding to TS ie. year to evaluated per loop 
            ## and cols not with dissimilarity corresponding to the TS rows
            ## each row should correspond to one spatial area
            S.dd[ 1 : n.ts, (n.ts+1) : (n.ts+n.fs) ],
            nrow = n.ts,
            ncol = length((n.ts+1) : (n.ts+n.fs))
          )
          colnames(S.dd) <- rownames(FS.a)
          rownames(S.dd) <- rownames(TS.a)
          
        } else if(method == "EDlog"){
          ## Euclidean distance dissimilarity 
          ## based on the stock size of the area
          
          S.a1 <- S.a
          for(i in 1:ncol(S.a)){ 
            ## for to reduce to the log and divide by the number of variables
            ## to avoid the influence of bigger dataset
            for(j in 1:nrow(S.a)){
              S.a1[j,i] <- log(S.a[j,i]+1)
            }
          }
          S.d1 <- as.matrix(dist(S.a1, method = "euclidean"))
          
          S.dd <- S.d1 
          S.dd <- matrix(
            S.dd[ 1 : n.ts, (n.ts+1) : (n.ts+n.fs) ], 
            nrow = n.ts, 
            ncol = length((n.ts+1) : (n.ts+n.fs))
          )
          colnames(S.dd) <- rownames(FS.a)
          rownames(S.dd) <- rownames(TS.a)
        }
        
        ## make a structure 'close' that will contain various attributes, 
        ## used in evaluating closest analogue
        close <- vector("list", n.ts)
        for(i in seq_len(n.ts)){
          if(is.matrix(S.dd)){
            close[[i]] <- sort(S.dd[i,])
          } else {
            close[[i]] <- sort(S.dd[i])
          }
        }
        names(close) <- names.ts 
        
        Fuse.cma <- list(close)
        names(Fuse.cma)[1] <- "close"
        
        ## cma 'close modern analogues' function is from the 'analogue' R package
        .call <- match.call()
        .call[[1]] <- as.name("cma")
        structure(
          list(
            close = close,
            call = .call, 
            quant = "none", 
            probs = "none",
            method = "fuse",
            n.analogs = "all"
          ),
          class = "cma"
        )
        class(Fuse.cma) <- "cma"
        Ana.cma <- Fuse.cma
        
        ## Extract the cma distance for each TS sample
        ## and writes them in data.frame Close.dat
        Close.dat <- data.frame(matrix(
          NA, nrow = 0, ncol = 5,
          dimnames = list(NULL, c(
            "cma_dist", "TS.ind", "FS.ind",
            "TS.area", "TS.year"
          ))
        ))
        TS.ind <- row.names(TS) 
        TS.n <- length(Ana.cma$close)
        
        ## for each Area in TS, make a dataframe with the dissimilarity,
        ## as well as TS and FS indices and areas
        for(i in 1:TS.n){
          k <- TS.ind[i]
          if(length(Ana.cma[["close"]][[k]]) > 0){
            ## first entry of each Ana.cma[["close"]] entry is the smallest distance
            temp <- as.data.frame(Ana.cma[["close"]][[k]][1])
            colnames(temp)[1] <- "cma_dist"
            temp$TS.ind <- as.numeric(TS.ind[i])
            
            if(nrow(FS) == 1){
              temp$FS.ind <- as.numeric(row.names(FS))
            } else {
              temp$FS.ind <- as.numeric(row.names(temp))
            }
            temp$TS.area <- TS[i,"Area"]
            temp$TS.year <- TS[i,"Year"]
            Close.dat <- rbind(Close.dat, temp)
          }
        }
        row.names(Close.dat) <- seq(nrow(Close.dat))
        FS.ind <- row.names(FS)    
        
        for(i in 1:nrow(Close.dat)){
          k <- as.numeric(Close.dat$FS.ind[i])
          Close.dat$FS.area[i] <- as.character(FS[which(rownames(FS) == k), "Area"])
          Close.dat$FS.year[i] <- FS[which(rownames(FS) == k), "Year"]
        }
        
        Ana_result <- rbind(Ana_result, Close.dat)
      }
    }
  }
  
  return(Ana_result)
}
```

```{r novelty staircase plotting function}
#| code-fold: true
#| code-summary: Staircase Plots of Novelty and Change
#| label: novelty-staircase-plotting-function

staircase_plot <- function(novelty_results){
  mnYr <- min(novelty_results$Year)
  mxYr <- max(novelty_results$Year)
  
  plot <- ggplot(novelty_results) + 
    geom_tile(aes(x = TS.year, y = Year, fill = cma_dist), color = "black") +
    scale_fill_viridis_c(option = "plasma", direction = -1) +
    labs(x = NULL, y = "Baseline", fill = "Novelty") +
    scale_x_continuous(
      breaks = seq(mnYr, mxYr+4, 2),
      expand = c(0,0),
      sec.axis = dup_axis()
    ) +
    scale_y_continuous(
      breaks = seq(mnYr, mxYr, 2),
      expand = c(0,0),
      sec.axis = dup_axis()
    ) +
    theme_bw() +
    theme(
      legend.key.height = unit(1.5,'cm'),
      legend.key.width = unit(0.3,'cm'),
      axis.text.x.top = element_blank(), 
      axis.text.y.right = element_blank(), 
      axis.title.x.top = element_blank(),
      axis.title.y.right = element_blank()
    ) +
    facet_grid(cols = vars(TS.area))
  
  return(plot)
}

```

## Data

```{r load cwm and species biomass}
#| code-fold: true
#| code-summary: Load CWM and Species Biomass Data
#| label: load-data

cwm_traits_data <- file.path(data_dir, "Multitrophic_CWM_traits_BalticSea.csv")
## note: these biomass data are survey/observation data
## may need to do some modeling to get better picture of actual spp biomasses
## (undersampling bias?)
biomass_data0 <- file.path(data_dir, "Pecuchet_Surveys_BS0.csv")
biomass_data <- file.path(data_dir, "Pecuchet_BSsurvey.csv")

if(!file.exists(biomass_data)){
  library(stringr)
  
  ## read with quotation marks to be able to distinguish columns
  tmp <- read.table(biomass_data0, header = TRUE, sep = "\t", quote = "")
  colnames(tmp) <- "allsquashedtogether"
  
  tmp2 <- tmp |> 
    rowwise() |> 
    ## put NAs in quotes so can split
    mutate(allsquashedtogether = str_replace_all(
      allsquashedtogether, "NA", "\"NA\""
    )) |> 
    mutate(spl1 = str_split(allsquashedtogether, "\"\\s")) |> 
    mutate(spl2 = ifelse(
      str_detect(spl1[6], "^[0-9]{2}\\.[0-9]+"),
      paste(spl1[6], spl1[7], spl1[8]),
      paste(spl1[6], spl1[7], spl1[8], spl1[9], spl1[10], spl1[11])
    )) |> 
    mutate(spl2 = str_split(spl2, "\\s")) |> 
    ## extract information for the columns we want
    ## hope its all correct...
    mutate(
      Survey = spl1[3],
      Area = spl1[4],
      SD = spl1[5],
      lat = spl2[1],
      lon = spl2[2],
      Depth = spl2[3],
      Year = spl2[4],
      Month = spl2[5],
      Taxa = spl2[6],
      Abundance = spl2[7],
      Biomass = spl2[8],
      AphiaID_accepted = spl2[9],
      ScientificName_accepted = spl1[length(spl1)-6]
    )
  tmp3 <- tmp2 |> 
    ungroup() |> 
    select(
      Survey, Area, SD, Taxa, AphiaID_accepted, ScientificName_accepted,
      lat, lon, Depth, Year, Month, Abundance, Biomass
    ) |> 
    mutate(across(1:13,  ~str_remove_all(.x, "\""))) |> 
    mutate(across(1:13, ~ifelse(.x == "NA", NA, .x))) |> 
    mutate(across(7:13, ~as.numeric(.x)))
  
  # ggplot(filter(tmp3, !is.na(Biomass), !Taxa == "Benthos_severalst")) + 
  #  geom_bar(aes(x = Year, fill = Area), position = position_stack()) + 
  #  facet_wrap(~Taxa, scales = "free_y")
    
  ## save as a standard csv file
  write.csv(tmp3, biomass_data)
}

## for now we just use Gulf of Riga
df0_traits <- read.csv(cwm_traits_data) |> 
  filter(Area == "GoR")
df0_biomass <- read.csv(biomass_data) |> 
  ## many Benthos and Phytoplankton rows at top of table lack Species info!!
  filter(!is.na(ScientificName_accepted)) |> 
  filter(Area %in% c("43H4","46H1","44H2","44H3","45H2","45H3","Gulf_Riga",NA)) |> 
  filter(lon > 20) |> 
  mutate(Area = "GoR")

## which years apply for (all) basin(s) being analyzed
useYrs <- 1979:2016
startYrs <- useYrs[5:length(useYrs)-5]
useMinYr <- min(useYrs) - 1
```

```{r functions to wrangle data}
#| code-fold: true
#| code-summary: Function to Wrangle data 
#| label: wrangling-data

## make a function because this will be done 8 times...
## 4 taxa groups (benthos, fish, zooplankton, phytoplankton) 
## 2 calculations, on traits and species biomass

## useYrs and useMinYr defined above in load-data code chunk
## function to fill NA using linear trend
fill_trend <- function(x, idx){
  res <- x[idx]
  if(is.na(res)){
    not_na <- which(!is.na(x))
    if(length(not_na) >= 2){
      x <- x[not_na]
      res <- predict(lm(x ~ not_na), data.frame(not_na = idx))[[1]]
    }
  }
  return(res)
}

## four options for fun argument:
## bins5yr will create discrete bins, so data looks like steps with 5yr width
## roll5yr will take rolling mean across all values, filling in NAs in 5yr window
## gfroll5yr replaces (not necessarily all) NAs with a value from rolling mean
## gftrend replaces only (not all) NAs with value from (5yr window) lm-trend
wrangle_data <- function(df0, x, taxa, fun){
  ## CWM option is for traits/ functional novelty analysis
  ## Biomass dataset is surveys (later change to pre-processed/modeled data...?)
  if(x == "CWM"){
    df <- df0 |> 
      filter(Taxa %in% taxa) |> 
      select(Area, Year, Factor = Trait, Value) |> 
      mutate(across(c("Year", "Value"), as.numeric))
  }
  if(x == "Biomass"){
    df <- df0 |> 
      filter(Taxa == taxa) |>
      group_by(Area, Year, Month, ScientificName_accepted) |> 
      ## what to do about depths?
      ## at this point we just take per month averages over depths
      summarize(Value = mean(Biomass, na.rm = TRUE)) |> 
      rename(Factor = ScientificName_accepted) 
  }
  
  if(fun == "bins5yr"){
    ## 5-year bin averages
    df_yrs <- df |> 
      filter(Year %in% useYrs) |> 
      mutate(Bin = ceiling((Year-useMinYr)/5)) |> 
      group_by(Bin) |> 
      mutate(maxYear = max(Year)) |> 
      group_by(Bin, Area) |> 
      mutate(nYears = n_distinct(Year))

    if("Month" %in% names(df_yrs)){
      ## if multiple months, which to use...
      ## pick the month(s) most often occuring across year-bins??
      # df_yrs <- df_yrs |>
      #   group_by(Area, Factor, Month) |>
      #   mutate(nBins = n_distinct(Bin)) |>
      #   group_by(Area, Factor) |>
      #   mutate(mooMonth = nBins == max(nBins)) |>
      #   ungroup() |>
      #   filter(mooMonth) |>
      #   mutate(Factor = paste(Factor, Month))
      if(taxa %in% c("Phytoplankton", "Zooplankton")){
        df_yrs <- df_yrs |>
          mutate(Factor = case_when(
            Month %in% 3:5 ~ paste(Factor, "- Spring"),
            Month %in% 6:8 ~ paste(Factor, "- Summer"),
            Month %in% 9:11 ~ paste(Factor, "- Autumn")
          ))
      }
    }
    df_long <- df_yrs |> 
      select(Area, Factor, Bin, Year, maxYear, Value) |> 
      arrange(Area, Factor, Bin) |> 
      group_by(Area, Factor, Bin) |> 
      mutate(Value = mean(Value)) |> 
      ungroup()
    FactorLevels <- unique(df_long$Factor)
    df_long$Factor <- factor(df_long$Factor, levels = FactorLevels)
    
    ## expand so have missing years filled with same 5-year bin average
    ## in many cases 5-year bin averages are based on fewer than 5 values
    ## because of missing data
    df_short <- df_long |> 
      distinct(Area, Factor, maxYear, Value) |> 
      rowwise() |> 
      ## bc year list only works if nyears is mod5...
      mutate(maxYear = ifelse(
        maxYear == max(df$Year),
        maxYear + diff(range(df$Year)) %% 5 - 1,
        maxYear
      )) |> 
      mutate(Year = list((maxYear-4):maxYear)) |> 
      ungroup() |> 
      unnest(cols = c(Year)) |> 
      pivot_wider(
        names_from = "Factor", 
        values_from = "Value"
      )
  }
  
  if(fun %in% c("roll5yr", "gfroll5yr", "gftrend")){
    if("Month" %in% names(df)){
      ## if multiple months, which to use...
      ## pick the month(s) most often occuring across all years
      # df <- df |>
      #   group_by(Area, Factor, Month) |>
      #   mutate(nYrs = ifelse(is.na(Month), 0, n_distinct(Year))) |>
      #   group_by(Area, Factor) |>
      #   mutate(mooMonth = nYrs == max(nYrs)) |>
      #   ungroup() |>
      #   filter(mooMonth)
      if(taxa %in% c("Phytoplankton", "Zooplankton")){
        df <- df |>
          mutate(Factor = case_when(
            Month %in% 3:5 ~ paste(Factor, "- Spring"),
            Month %in% 6:8 ~ paste(Factor, "- Summer"),
            Month %in% 9:11 ~ paste(Factor, "- Autumn")
          ))
      }
    }
    FactorLevels <- unique(df$Factor)
    allYrs <- seq(min(df$Year), max(df$Year))
    
    df <- expand.grid(Area=unique(df$Area), Factor=FactorLevels, Year=allYrs) |> 
      left_join(df, by = join_by(Area, Factor, Year)) |>
      group_by(Area, Factor, Year) |> 
      summarize(Value = mean(Value, na.rm = TRUE)) |> 
      arrange(Area, Factor, Year) |> 
      group_by(Area, Factor)
  
    ## 5-year rolling means
    ## CWM data are already rolling means so should only fill with trend???
    if(fun == "roll5yr"){
      df_long <- df |> 
        mutate(Value = rollmean(
          Value, 5, fill = NA,
          align = "right",
          na.rm = TRUE
        )) 
    }
    if(fun == "gfroll5yr"){
      df_long <- df |> 
        mutate(meanFill = rollmean(
          Value, 5, fill = NA,
          align = "right",
          na.rm = TRUE
        )) |> 
        mutate(Value = ifelse(
          is.na(Value), 
          meanFill, Value
        ))
    }
    if(fun == "gftrend"){
      df_long <- df |> 
        mutate(trendFill = rollapply(
          Value, 5, fill = NA,
          FUN = function(x){fill_trend(x, 3)},
          align = "center"
        )) |> 
        ## if still have NAs, add right aligned trend...
        mutate(trendFill2 = rollapply(
          Value, 5, fill = NA,
          FUN = function(x){fill_trend(x, 5)},
          align = "right"
        )) |> 
        mutate(Value = ifelse(
          is.na(Value), 
          trendFill, Value
        )) |> 
        mutate(Value = ifelse(
          is.na(Value),
          trendFill2, Value
        ))
    }
    df_long <- df_long |> 
      filter(Year %in% useYrs) |> 
      select(Area, Factor, Year, Value) |> 
      ## set abundance value rolling means less than zero to zero
      ## (why are there means less than zero??)
      mutate(Value = ifelse(Value > 0, Value, 0)) |> 
      ungroup() |> 
      filter(Year %in% useYrs[5:length(useYrs)]) |> 
      mutate(Bin = ceiling((Year-useMinYr)/5)) |> 
      group_by(Bin) |> 
      mutate(maxYear = max(Year))
    
    df_long$Factor <- factor(df_long$Factor, levels = FactorLevels)
    
    df_short <- df_long |> 
      pivot_wider(names_from = "Factor", values_from = "Value") |> 
      ungroup()
  }
  # View(sort(colSums(is.na(df_short))))
  
  ## plots of the data, to check missing data etc
  p <- ggplot(df_long, aes(x = Year, y = Value, color = Area)) + 
    geom_point(size = 0.5, alpha = 0.5) +
    geom_vline(aes(xintercept = maxYear), linewidth = 0.1) +
    facet_wrap(~Factor, scales = "free_y", nrow = 4) +
    labs(x = "\nYear") +
    theme(legend.position = "bottom")
  
  if(fun == "bin5yr"){
    p <- p + 
      geom_point(shape = 95, size = 4) +
      labs(y = paste(x, "5-Year Bin Averages"))
  }
  if(fun == "gfroll5yr"){
    p <- p + 
      geom_line(linewidth = 0.2) +
      labs(y = paste(x, "Gapfilled 5-Year Rolling Averages"))
  }
  if(fun == "gftrend"){
    p <- p + 
      geom_line(linewidth = 0.2) + 
      labs(y = paste(x, "Gapfilled with 5-Year Window Trend"))
  }
  
  ## return data and plot
  ## long data only used to make plots,
  ## short data used to do the novelty calculation
  return(list(data = df_short, plot = p))
}
```

## Biotic Novelty – CWM Traits

-   will use Biomass now, still using Hellinger distance

-   testing just Gulf of Riga (so will effectively be just within-basin change/dissimilarities)

-   will calculate the novelty for each species group individually

-   then also calculate novelty for all species together, test whether method is sensitive enough

```{r wrangle cwm traits data}
#| code-fold: true
#| code-summary: Wrangle Traits Data
#| label: wrangle-cwm-traits-data

AllTaxa <- c("Phytoplankton", "Zooplankton", "Benthos", "Fish")

df_traits <- df0_traits |> 
  filter(Variable == "biomass") |> 
  # filter(Variable == "abundance") |> 
  wrangle_data("CWM", "Phytoplankton", "gftrend")
  # wrangle_data("CWM", "Zooplankton", "gftrend")
  # wrangle_data("CWM", "Benthos", "gftrend")
  # wrangle_data("CWM", "Fish", "gftrend")
  # wrangle_data("CWM", AllTaxa, "gftrend")

# View(df_traits$data)
df_traits$plot
```

```{r degree of novelty traits}
#| code-fold: true
#| code-summary: Degree of Novelty in Traits
#| label: degree-of-novelty-traits

traits_novelty_list <- lapply(
  startYrs, FUN = function(x){
    Novelty_function(
      M = df_traits$data,
      start_baseline = x,
      end_baseline = (x+4),
      end_year = max(useYrs),
      method = "Hellinger"
    )
  }
)
names(traits_novelty_list) <- startYrs
traits_novelty <- traits_novelty_list |> 
  bind_rows(.id = "Year") |> 
  mutate(Year = as.numeric(Year))

## staircase plot of novelty
staircase_plot(traits_novelty)
```

```{r within basin traits change}
#| code-fold: true
#| code-summary: Rate of Change
#| label: within-basin-traits-change

## (distance dissimilarity calculation restricted to within-basin)
traits_change <- data.frame(matrix(
  NA, nrow = 0, ncol = 8,
  dimnames = list(NULL, c(
    "Year", "cma_dist", "TS.ind", "FS.ind", 
    "TS.area", "TS.year", "FS.area", "FS.year"
  ))
))
for(i in unique(df_traits$data$Area)){
  data <- df_traits$data[which(df_traits$data$Area==i),]
  
  tmplst <- lapply(startYrs, FUN = function(x){
    Novelty_function(
      M = data, 
      start_baseline = x,
      end_baseline = (x+4),
      end_year = max(useYrs), 
      method = "Hellinger"
    )
  })
  names(tmplst) <- startYrs
  
  traits_change <- tmplst |> 
    bind_rows(.id = "Year") |> 
    rbind(traits_change) |> 
    mutate(Year = as.numeric(Year))
}
staircase_plot(traits_change)
```

## Biotic Novelty – Species Biomass

-   in order to test for the difference between shifts in traits versus species, e.g. is the basin showing little trait-novelty because of redundancy in species, but higher species composition novelty?

-   currently using survey data but may want to use modeled data to e.g. correct for undersampling bias, distinguish explicitly between NAs and zeros, etc.

-   for taxa with strong seasonality (plankton), spring and fall may be considered as separate factors in the novelty dissimilarity calculation, but for others we will use an annual average

```{r wrangle species compositions biomass data}
#| code-fold: true
#| code-summary: Wrangle Species Data
#| label: wrangle-species-data


df_spp <- df0_biomass |> 
  ## better to use roll5yr rather than just gfroll5yr??
  # wrangle_data("Biomass", "Phytoplankton", "gfroll5yr")
  wrangle_data("Biomass", "Zooplankton", "gfroll5yr")
  # wrangle_data("Biomass", "Benthos", "gfroll5yr")
  # wrangle_data("Biomass", "Fish", "gfroll5yr")

cs <- colSums(is.na(df_spp$data))
df_spp$data <- select(df_spp$data, all_of(names(cs[which(cs == 0)])))

## lots of phytoplankton species, maybe don't keep the less common ones??
# cs <- colSums(select(df_spp$data, matches("Spring|Summer")))
# df_spp$data <- df_spp$data |> 
#   select(
#     Area, Year, Bin, maxYear,
#     all_of(names(cs[which(cs > 0.1)]))
#   )

# View(df_spp$data)
# df_spp$plot
```

```{r degree of novelty species}
#| code-fold: true
#| code-summary: Degree of Novelty in Species
#| label: degree-of-novelty-species

## note: avoid running for phytoplankton 
## with the about 200species it takes forever...
traits_novelty_list <- lapply(
  startYrs, FUN = function(x){
    Novelty_function(
      M = df_spp$data,
      start_baseline = x,
      end_baseline = (x+4),
      end_year = max(useYrs),
      method = "Hellinger"
    )
  }
)
names(traits_novelty_list) <- startYrs
traits_novelty <- traits_novelty_list |> 
  bind_rows(.id = "Year") |> 
  mutate(Year = as.numeric(Year))

## staircase plot of novelty
staircase_plot(traits_novelty)
```
